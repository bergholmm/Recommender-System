{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read dataset\n",
      "creating ICM\n",
      "(99999, 31900)\n",
      "(100000, 31900)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<100000x31900 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 483482 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def readTrainingSet():\n",
    "    file = open('../data/train_final.csv', 'r')\n",
    "    ds = []\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        split = line.split(\"\\t\")\n",
    "        split[1] = split[1].replace(\"\\n\",\"\")\n",
    "        split[0] = int(split[0])\n",
    "        split[1] = int(split[1])\n",
    "        ds.append(tuple(split))\n",
    "\n",
    "    file.close()\n",
    "    return ds\n",
    "\n",
    "def readItemInfo():\n",
    "    file = open('../data/tracks_final.csv', 'r')\n",
    "    ds = []\n",
    "    missing =[]\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        split = line.split(\"\\t\")\n",
    "        split[5] = split[5].replace(\"\\n\",\"\")\n",
    "        split[5] = split[5].replace(\"[\",\"\")\n",
    "        split[5] = split[5].replace(\"]\",\"\")\n",
    "        split[5] = split[5].replace(\",\",\"\")\n",
    "        tags = split[5].split()\n",
    "\n",
    "        if len(tags) == 0:\n",
    "            missing.append(int(split[0]))\n",
    "        else:\n",
    "            for tag in tags:\n",
    "                ds.append((int(split[0]), int(tag)))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    songWithTags, tags = zip(*ds)\n",
    "    songWithTags = list(songWithTags)\n",
    "    tags = list(tags)\n",
    "\n",
    "    # Preprocess songs\n",
    "    les = preprocessing.LabelEncoder()\n",
    "    allSongs = songWithTags + missing\n",
    "    les.fit(allSongs)\n",
    "    songWithTags = les.transform(songWithTags)\n",
    "\n",
    "    # Preprocess tags\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(tags)\n",
    "    tags = le.transform(tags)\n",
    "    indices = np.ones(len(tags))\n",
    "\n",
    "    print('creating ICM')\n",
    "    ICM = sps.coo_matrix((indices, (songWithTags, tags))).tocsc()\n",
    "\n",
    "    print(ICM.shape)\n",
    "    missingItems = np.zeros((1, len(set(tags))))\n",
    "    missingItems = sps.csc_matrix(missingItems)\n",
    "    ICM = sps.vstack((ICM, missingItems)).tocsc()\n",
    "    print(ICM.shape)\n",
    "\n",
    "    # return ICM.tocsr()\n",
    "    # return ICM.tocsc(), les\n",
    "    return ICM, les\n",
    "\n",
    "\n",
    "def readTargetPlaylists():\n",
    "    return np.genfromtxt('../data/target_playlists.csv', delimiter='\\t', skip_header=1, dtype=int)\n",
    "\n",
    "\n",
    "def splitTestTrainDS(ds, les):\n",
    "    playlists, songs = zip(*ds)\n",
    "    ps = np.array(playlists)\n",
    "    s = list(songs)\n",
    "    s = np.array(les.transform(s))\n",
    "    rating = np.ones(s.size)\n",
    "\n",
    "    train_split = 0.8\n",
    "    num_interactions = len(ds)\n",
    "\n",
    "    mask = np.random.choice([True, False], num_interactions, p=[train_split, 1-train_split])\n",
    "    trainds = sps.coo_matrix((rating[mask], (ps[mask], s[mask]))).tocsr()\n",
    "    mask = np.logical_not(mask)\n",
    "    testds = sps.coo_matrix((rating[mask], (ps[mask], s[mask]))).tocsr()\n",
    "\n",
    "    return trainds, testds\n",
    "\n",
    "\n",
    "def readData():\n",
    "    ICM, les = readItemInfo()\n",
    "    ds = readTrainingSet()\n",
    "\n",
    "    trainingSet, testSet = splitTestTrainDS(ds, les)\n",
    "\n",
    "    playlists, _ = zip(*ds)\n",
    "    targets = list(set(playlists))\n",
    "\n",
    "    return trainingSet, testSet, targets, ICM, les\n",
    "\n",
    "\n",
    "print('Read dataset')\n",
    "\n",
    "trainds, testds, targets, ICM, les = readData()\n",
    "ICM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Normalized\n",
      "Computed\n",
      "Removed diagonal\n",
      "done i compute\n",
      "dist done\n",
      "Converted to csr\n",
      "Item 0 of 100000\n",
      "Item 1000 of 100000\n",
      "Item 2000 of 100000\n",
      "Item 3000 of 100000\n",
      "Item 4000 of 100000\n",
      "Item 5000 of 100000\n",
      "Item 6000 of 100000\n",
      "Item 7000 of 100000\n",
      "Item 8000 of 100000\n",
      "Item 9000 of 100000\n",
      "Item 10000 of 100000\n",
      "Item 11000 of 100000\n",
      "Item 12000 of 100000\n",
      "Item 13000 of 100000\n",
      "Item 14000 of 100000\n",
      "Item 15000 of 100000\n",
      "Item 16000 of 100000\n",
      "Item 17000 of 100000\n",
      "Item 18000 of 100000\n",
      "Item 19000 of 100000\n",
      "Item 20000 of 100000\n",
      "Item 21000 of 100000\n",
      "Item 22000 of 100000\n",
      "Item 23000 of 100000\n",
      "Item 24000 of 100000\n",
      "Item 25000 of 100000\n",
      "Item 26000 of 100000\n",
      "Item 27000 of 100000\n",
      "Item 28000 of 100000\n",
      "Item 29000 of 100000\n",
      "Item 30000 of 100000\n",
      "Item 31000 of 100000\n",
      "Item 32000 of 100000\n",
      "Item 33000 of 100000\n",
      "Item 34000 of 100000\n",
      "Item 35000 of 100000\n",
      "Item 36000 of 100000\n",
      "Item 37000 of 100000\n",
      "Item 38000 of 100000\n",
      "Item 39000 of 100000\n",
      "Item 40000 of 100000\n",
      "Item 41000 of 100000\n",
      "Item 42000 of 100000\n",
      "Item 43000 of 100000\n",
      "Item 44000 of 100000\n",
      "Item 45000 of 100000\n",
      "Item 46000 of 100000\n",
      "Item 47000 of 100000\n",
      "Item 48000 of 100000\n",
      "Item 49000 of 100000\n",
      "Item 50000 of 100000\n",
      "Item 51000 of 100000\n",
      "Item 52000 of 100000\n",
      "Item 53000 of 100000\n",
      "Item 54000 of 100000\n",
      "Item 55000 of 100000\n",
      "Item 56000 of 100000\n",
      "Item 57000 of 100000\n",
      "Item 58000 of 100000\n",
      "Item 59000 of 100000\n",
      "Item 60000 of 100000\n",
      "Item 61000 of 100000\n",
      "Item 62000 of 100000\n",
      "Item 63000 of 100000\n",
      "Item 64000 of 100000\n",
      "Item 65000 of 100000\n",
      "Item 66000 of 100000\n",
      "Item 67000 of 100000\n",
      "Item 68000 of 100000\n",
      "Item 69000 of 100000\n",
      "Item 70000 of 100000\n",
      "Item 71000 of 100000\n",
      "Item 72000 of 100000\n",
      "Item 73000 of 100000\n",
      "Item 74000 of 100000\n",
      "Item 75000 of 100000\n",
      "Item 76000 of 100000\n",
      "Item 77000 of 100000\n",
      "Item 78000 of 100000\n",
      "Item 79000 of 100000\n",
      "Item 80000 of 100000\n",
      "Item 81000 of 100000\n",
      "Item 82000 of 100000\n",
      "Item 83000 of 100000\n",
      "Item 84000 of 100000\n",
      "Item 85000 of 100000\n",
      "Item 86000 of 100000\n",
      "Item 87000 of 100000\n",
      "Item 88000 of 100000\n",
      "Item 89000 of 100000\n",
      "Item 90000 of 100000\n",
      "Item 91000 of 100000\n",
      "Item 92000 of 100000\n",
      "Item 93000 of 100000\n",
      "Item 94000 of 100000\n",
      "Item 95000 of 100000\n",
      "Item 96000 of 100000\n",
      "Item 97000 of 100000\n",
      "Item 98000 of 100000\n",
      "Item 99000 of 100000\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "class ISimilarity(object):\n",
    "    \"\"\"Abstract interface for the similarity metrics\"\"\"\n",
    "\n",
    "    def __init__(self, shrinkage=10):\n",
    "        self.shrinkage = shrinkage\n",
    "\n",
    "    def compute(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cosine(ISimilarity):\n",
    "    def compute(self, X):\n",
    "        # convert to csc matrix for faster column-wise operations\n",
    "        X = check_matrix(X, 'csc', dtype=np.float32)\n",
    "\n",
    "        # 1) normalize the columns in X\n",
    "        # compute the column-wise norm\n",
    "        # NOTE: this is slightly inefficient. We must copy X to compute the column norms.\n",
    "        # A faster solution is to  normalize the matrix inplace with a Cython function.\n",
    "        Xsq = X.copy()\n",
    "        Xsq.data **= 2\n",
    "        norm = np.sqrt(Xsq.sum(axis=0))\n",
    "        norm = np.asarray(norm).ravel()\n",
    "        norm += 1e-6\n",
    "        # compute the number of non-zeros in each column\n",
    "        # NOTE: this works only if X is instance of sparse.csc_matrix\n",
    "        col_nnz = np.diff(X.indptr)\n",
    "        # then normalize the values in each column\n",
    "        X.data /= np.repeat(norm, col_nnz)\n",
    "        print(\"Normalized\")\n",
    "\n",
    "        # 2) compute the cosine similarity using the dot-product\n",
    "        dist = X * X.T\n",
    "        print(\"Computed\")\n",
    "\n",
    "        # zero out diagonal values\n",
    "        dist = dist - sps.dia_matrix((dist.diagonal()[scipy.newaxis, :], [0]), shape=dist.shape)\n",
    "        print(\"Removed diagonal\")\n",
    "\n",
    "        # and apply the shrinkage\n",
    "        if self.shrinkage > 0:\n",
    "            dist = self.apply_shrinkage(X, dist)\n",
    "            print(\"Applied shrinkage\")\n",
    "\n",
    "        print('done i compute')\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def apply_shrinkage(self, X, dist):\n",
    "        # create an \"indicator\" version of X (i.e. replace values in X with ones)\n",
    "        X_ind = X.copy()\n",
    "        X_ind.data = np.ones_like(X_ind.data)\n",
    "        # compute the co-rated counts\n",
    "        co_counts = X_ind * X_ind.T\n",
    "        # remove the diagonal\n",
    "        co_counts = co_counts - sps.dia_matrix((co_counts.diagonal()[scipy.newaxis, :], [0]), shape=co_counts.shape)\n",
    "        # compute the shrinkage factor as co_counts_ij / (co_counts_ij + shrinkage)\n",
    "        # then multiply dist with it\n",
    "        co_counts_shrink = co_counts.copy()\n",
    "        co_counts_shrink.data += self.shrinkage\n",
    "        co_counts.data /= co_counts_shrink.data\n",
    "        dist.data *= co_counts.data\n",
    "        return dist\n",
    "\n",
    "\n",
    "class BasicItemKNNRecommender(object):\n",
    "    \"\"\" ItemKNN recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM, k=50, shrinkage=15, similarity='cosine'):\n",
    "        self.dataset = URM\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_name = similarity\n",
    "        if similarity == 'cosine':\n",
    "            self.distance = Cosine(shrinkage=self.shrinkage)\n",
    "        # elif similarity == 'pearson':\n",
    "        #     self.distance = Pearson(shrinkage=self.shrinkage)\n",
    "        # elif similarity == 'adj-cosine':\n",
    "        #     self.distance = AdjustedCosine(shrinkage=self.shrinkage)\n",
    "        else:\n",
    "            raise NotImplementedError('Distance {} not implemented'.format(similarity))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(\n",
    "            self.similarity_name, self.k, self.shrinkage)\n",
    "\n",
    "    def fit(self, X):\n",
    "        item_weights = self.distance.compute(X)\n",
    "        print('dist done')\n",
    "        item_weights = check_matrix(item_weights, 'csr') # nearly 10 times faster\n",
    "        print(\"Converted to csr\")\n",
    "\n",
    "        # for each column, keep only the top-k scored items\n",
    "        # THIS IS THE SLOW PART, FIND A BETTER SOLUTION\n",
    "        values, rows, cols = [], [], []\n",
    "        nitems = self.dataset.shape[1]\n",
    "        for i in range(nitems):\n",
    "            if (i % 10000 == 0):\n",
    "                print(\"Item %d of %d\" % (i, nitems))\n",
    "\n",
    "            this_item_weights = item_weights[i,:].toarray()[0]\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "\n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(nitems)[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * i)\n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)), shape=(nitems, nitems), dtype=np.float32)\n",
    "\n",
    "    def recommend(self, user_id, at=5, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "\n",
    "        return ranking[:at]\n",
    "\n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]\n",
    "    \n",
    "    \n",
    "recommender = BasicItemKNNRecommender(trainds)\n",
    "print('Fit model')\n",
    "recommender.fit(ICM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate recommender\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-576166497b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluate recommender'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testds' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "def evaluate(ds, targets, recommender):\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    numTargets = len(targets)\n",
    "\n",
    "    for t in targets:\n",
    "        if num_eval % 100 == 0:\n",
    "            print('targets done: ', num_eval, numTargets)\n",
    "\n",
    "        rel_songs = ds[t].indices\n",
    "\n",
    "        if len(rel_songs) > 0:\n",
    "            rec_songs = recommender.recommend(t)\n",
    "            num_eval += 1\n",
    "            cumulative_precision += precision(rec_songs, rel_songs)\n",
    "            cumulative_recall += recall(rec_songs, rel_songs)\n",
    "            cumulative_MAP += MAP(rec_songs, rel_songs)\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "    cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "        \n",
    "print('Evaluate recommender')\n",
    "evaluate(testds, targets, recommender)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict(targets, recommender, les):\n",
    "    recommendations = []\n",
    "    i = 0\n",
    "    numTargets = len(targets)\n",
    "    for t in targets:\n",
    "        if i % 1000 == 0:\n",
    "            print('targets done: ', i, numTargets)\n",
    "        rec = recommender.recommend(t)\n",
    "        res = les.inverse_transform(rec)\n",
    "        recommendations.append(' '.join(str(p) for p in res))\n",
    "        i += 1\n",
    "\n",
    "    result = list(zip(targets, recommendations))\n",
    "    t = int(time.time())\n",
    "    np.savetxt('../result/'+ str(t) +'.csv', result, fmt=\"%s\", delimiter=',', header='playlist_id,track_ids')\n",
    "    return recommendations\n",
    "\n",
    "def readTargetPlaylists():\n",
    "    return np.genfromtxt('../data/target_playlists.csv', delimiter='\\t', skip_header=1, dtype=int)\n",
    "\n",
    "def recommend(recommender, les):\n",
    "    t = readTargetPlaylists()\n",
    "    predict(t, recommender, les)\n",
    "\n",
    "print('Recommend')\n",
    "recommend(recommender, les)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
